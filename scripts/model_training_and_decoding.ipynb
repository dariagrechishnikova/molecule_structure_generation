{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 120
    },
    "colab_type": "code",
    "id": "Y_2jy3gUF_KV",
    "outputId": "e1e272a4-0dc2-4ad5-ee42-882d81716a48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# Run this cell to mount your Google Drive.\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8Nf_PgQ5ySJn"
   },
   "outputs": [],
   "source": [
    "! cp /content/drive/My\\ Drive/model.ckpt-600000* /tmp/t2t/output/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JaZ3aRRfyVO7"
   },
   "outputs": [],
   "source": [
    "% cd /content/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dpYz-i4fyYk-"
   },
   "outputs": [],
   "source": [
    "#upload data\n",
    "from google.colab import files\n",
    "files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zzO8UhPDq-mU"
   },
   "outputs": [],
   "source": [
    "!unzip prot_lig_binding_human_ds5.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -q -U tensor2tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X7fxvxhCfp3c"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Enable Eager execution - useful for seeing the generated data.\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "from tensor2tensor.utils import trainer_lib\n",
    "\n",
    "# Set a seed so that we have deterministic outputs.\n",
    "RANDOM_SEED = 301\n",
    "trainer_lib.set_random_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VaZFv_mcfwr6"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Setup and create directories.\n",
    "DATA_DIR = os.path.expanduser(\"/tmp/t2t/data\")\n",
    "OUTPUT_DIR = os.path.expanduser(\"/tmp/t2t/output\")\n",
    "TMP_DIR = os.path.expanduser(\"/tmp/t2t/tmp\")\n",
    "USR_DIR = os.path.expanduser(\"/tmp/t2t/usr\")\n",
    "\n",
    "# Create them.\n",
    "tf.gfile.MakeDirs(DATA_DIR)\n",
    "tf.gfile.MakeDirs(OUTPUT_DIR)\n",
    "tf.gfile.MakeDirs(TMP_DIR)\n",
    "tf.gfile.MakeDirs(USR_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ksKwM3jT-ynU"
   },
   "outputs": [],
   "source": [
    "! mkdir transformer_scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JGpkyVqwPenb"
   },
   "outputs": [],
   "source": [
    "% cd /tmp/t2t/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b_vJy_ljPeuV"
   },
   "outputs": [],
   "source": [
    "#upload vocab file\n",
    "from google.colab import files\n",
    "files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YAPW15g1PjeC"
   },
   "outputs": [],
   "source": [
    "% cd /content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9bBqYYHtAEfD"
   },
   "outputs": [],
   "source": [
    "%%writefile transformer_scratch/prob.py\n",
    "\n",
    "import re\n",
    "\n",
    "  \n",
    "  \n",
    "# `Problem` is the base class for any dataset that we want to add to T2T -- it\n",
    "# unifies the specification of the problem for generating training data,\n",
    "# training, evaluation and inference.\n",
    "#\n",
    "# All its methods (except `generate_data`) have reasonable default\n",
    "# implementations.\n",
    "#\n",
    "# A sub-class must implement `generate_data(data_dir, tmp_dir)` -- this method\n",
    "# is called by t2t-trainer or t2t-datagen to actually generate TFRecord dataset\n",
    "# files on disk.\n",
    "from tensor2tensor.data_generators import problem\n",
    "\n",
    "# Certain categories of problems are very common, like where either the input or\n",
    "# output is text, for such problems we define an (abstract) sub-class of\n",
    "# `Problem` called `Text2TextProblem` -- this implements `generate_data` in\n",
    "# terms of another function `generate_samples`. Sub-classes must override\n",
    "# `generate_samples` and `is_generate_per_split`.\n",
    "from tensor2tensor.data_generators import text_problems\n",
    "\n",
    "# Every non-abstract problem sub-class (as well as models and hyperparameter\n",
    "# sets) must be registered with T2T so that T2T knows about it and can look it\n",
    "# up when you specify your problem on the commandline to t2t-trainer or\n",
    "# t2t-datagen.\n",
    "#\n",
    "# One uses:\n",
    "# `register_problem` for a new Problem sub-class.\n",
    "# `register_model` for a new T2TModel sub-class.\n",
    "# `register_hparams` for a new hyperparameter set. All hyperparameter sets\n",
    "# typically extend `common_hparams.basic_params1` (directly or indirectly).\n",
    "from tensor2tensor.utils import registry\n",
    "\n",
    "\n",
    "# By default, when you register a problem (or model or hyperparameter set) the\n",
    "# name with which it gets registered is the 'snake case' version -- so here\n",
    "# the Problem class `ProteinSpecificLigandGeneration` will be registered with\n",
    "# the name `protein_specific_ligand_generation`.\n",
    "#\n",
    "# One can override this default by actually assigning a name as follows:\n",
    "# `@registry.register_problem(\"my_awesome_problem\")`\n",
    "#\n",
    "# The registered name is specified to the t2t-trainer or t2t-datagen using the\n",
    "# commandline flag `--problem`.\n",
    "@registry.register_problem('protein_specific_ligand_generation')\n",
    "\n",
    "# We inherit from `Text2TextProblem` which takes care of a lot of details\n",
    "# regarding reading and writing the data to disk, what vocabulary type one\n",
    "# should use, its size etc -- so that we need not worry about them, one can,\n",
    "# of course, override those.\n",
    "\n",
    "\n",
    "class ProteinSpecificLigandGeneration(text_problems.Text2TextProblem):\n",
    "  \n",
    "\n",
    "  # START: Methods we should override.\n",
    "\n",
    "  # The methods that need to be overriden from `Text2TextProblem` are:\n",
    "  # `is_generate_per_split` and\n",
    "  # `generate_samples`.\n",
    "\n",
    "  @property\n",
    "  def is_generate_per_split(self):\n",
    "    # If we have pre-existing data splits for (train, eval, test) then we set\n",
    "    # this to True, which will have generate_samples be called for each of the\n",
    "    # dataset_splits.\n",
    "    #\n",
    "    # If we do not have pre-existing data splits, we set this to False, which\n",
    "    # will have generate_samples be called just once and the Problem will\n",
    "    # automatically partition the data into dataset_splits.\n",
    "    return False\n",
    "\n",
    "  def generate_samples(self, data_dir, tmp_dir, dataset_split):\n",
    "    # Here we are generating the data in-situ using the `sample_sentence`\n",
    "    # function, otherwise we would have downloaded the data and put it in\n",
    "    # `tmp_dir` -- and read it from that location.\n",
    "    del tmp_dir\n",
    "\n",
    "    # Unused here, is used in `Text2TextProblem.generate_data`.\n",
    "    del data_dir\n",
    "\n",
    "    # This would have been useful if `self.is_generate_per_split()` was True.\n",
    "    # In that case we would have checked if we were generating a training,\n",
    "    # evaluation or test sample. This is of type `problem.DatasetSplit`.\n",
    "    del dataset_split\n",
    "\n",
    "    \n",
    "  #  drug_file = open (\"drugs.smi.txt\", \"r\")\n",
    "  \n",
    "  \n",
    "   \n",
    "    with open(\"/content/proteins_train.space_sep_seq\") as file1, open(\"/content/ligands_train.space_sep_seq\") as file2:\n",
    "      for x, y in zip(file1, file2):\n",
    "        inputs = x.strip()\n",
    "        targets = y.strip()\n",
    "        yield {\"inputs\":inputs,\"targets\":targets}\n",
    "     \n",
    "    \n",
    "    \n",
    "  # END: Methods we should override.\n",
    "\n",
    "  # START: Overridable methods.\n",
    "\n",
    "  @property\n",
    "  def vocab_type(self):\n",
    "    # We can use different types of vocabularies, `VocabType.CHARACTER`,\n",
    "    # `VocabType.SUBWORD` and `VocabType.TOKEN`.\n",
    "    #\n",
    "    # SUBWORD and CHARACTER are fully invertible -- but SUBWORD provides a good\n",
    "    # tradeoff between CHARACTER and TOKEN.\n",
    "    return text_problems.VocabType.TOKEN\n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "  @property\n",
    "  def dataset_splits(self):\n",
    "    # Since we are responsible for generating the dataset splits, we override\n",
    "    # `Text2TextProblem.dataset_splits` to specify that we intend to keep\n",
    "    # 80% data for training and 10% for evaluation and testing each.\n",
    "    return [{\n",
    "        \"split\": problem.DatasetSplit.TRAIN,\n",
    "        \"shards\": 196504,\n",
    "    }, {\n",
    "        \"split\": problem.DatasetSplit.EVAL,\n",
    "        \"shards\": 0,\n",
    "    }, {\n",
    "        \"split\": problem.DatasetSplit.TEST,\n",
    "        \"shards\": 0,\n",
    "    }]\n",
    "\n",
    " # END: Overridable methods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3CuAwlDVDL3f"
   },
   "outputs": [],
   "source": [
    "%%writefile transformer_scratch/__init__.py\n",
    "\n",
    "from transformer_scratch import prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3RMDmY3dELbM"
   },
   "outputs": [],
   "source": [
    "! t2t-datagen \\\n",
    "  --problem=protein_specific_ligand_generation \\\n",
    "  --data_dir=/tmp/t2t/data \\\n",
    "  --tmp_dir=/tmp/t2t/tmp \\\n",
    "  --t2t_usr_dir=/content/transformer_scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q40YyojYzJre"
   },
   "outputs": [],
   "source": [
    "! t2t-trainer \\\n",
    "  --model=transformer \\\n",
    "  --hparams_set=transformer_tiny \\\n",
    "  --hparams=\"batch_size=4096,max_length=0,label_smoothing=0,num_hidden_layers=4\" \\\n",
    "  --problem=protein_specific_ligand_generation \\\n",
    "  --worker_gpu=1 \\\n",
    "  --train_steps=5000000 \\\n",
    "  --data_dir=/tmp/t2t/data \\\n",
    "  --output_dir=/tmp/t2t/output \\\n",
    "  --t2t_usr_dir=/content/transformer_scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "oskaPzjVPuG6",
    "outputId": "841c99ec-1129-4984-f406-d2e1d1da090e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/t2t/data\n"
     ]
    }
   ],
   "source": [
    "%cd /tmp/t2t/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5f0yw7SnPyZd"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dkx5vqY9CiRE"
   },
   "outputs": [],
   "source": [
    "!unzip proteins_test.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "yNmc7olXP6jh",
    "outputId": "665625f7-24fa-47f9-8e7d-b4de216e5192"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content\n"
     ]
    }
   ],
   "source": [
    "% cd /content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N5REALoFEUfI"
   },
   "outputs": [],
   "source": [
    "from tensor2tensor.bin import t2t_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wh0sqNKIFXPg"
   },
   "outputs": [],
   "source": [
    "! t2t-decoder \\\n",
    "  --hparams_set=transformer_tiny \\\n",
    "  --hparams=\"batch_size=4096,max_length=0,label_smoothing=0,num_hidden_layers=4\" \\\n",
    "  --decode_hparams=\"beam_size=4,alpha=0.6,batch_size=4\" \\\n",
    "  --model=transformer \\\n",
    "  --problem=protein_specific_ligand_generation \\\n",
    "  --data_dir=/tmp/t2t/data \\\n",
    "  --output_dir=/tmp/t2t/output \\\n",
    "  --t2t_usr_dir=/content/transformer_scratch \\\n",
    "  --decode_from_file=/tmp/t2t/data/proteins_test5.space_sep_seq_unique \\\n",
    "  --decode_to_file=protein.seq.decode.results_bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cxGdamNZrWBs"
   },
   "outputs": [],
   "source": [
    "! t2t-decoder \\\n",
    "  --hparams_set=transformer_tiny \\\n",
    "  --hparams=\"batch_size=4096,max_length=0,label_smoothing=0,num_hidden_layers=4\" \\\n",
    "  --decode_hparams=\"beam_size=10,alpha=0.6,batch_size=4,write_beam_scores=False, return_beams=True\" \\\n",
    "  --model=transformer \\\n",
    "  --problem=protein_specific_ligand_generation \\\n",
    "  --data_dir=/tmp/t2t/data \\\n",
    "  --output_dir=/tmp/t2t/output \\\n",
    "  --t2t_usr_dir=/content/transformer_scratch \\\n",
    "  --decode_from_file=/tmp/t2t/data/proteins_test5.space_sep_seq_unique \\\n",
    "  --decode_to_file=protein.seq.decode.results_bs4_1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LOI41mt2FYi3"
   },
   "outputs": [],
   "source": [
    "%cd /tmp/t2t/output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YOSGh_T0f9FG"
   },
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZLrYWFCggXCf"
   },
   "outputs": [],
   "source": [
    "!rm checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_Doun1MlFFz6"
   },
   "outputs": [],
   "source": [
    "! cp model.ckpt-600000.* /content/drive/My\\ Drive/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zn2_HOqrDfJE"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CzY5TSorNdmx"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "adaL-_rHNeI9"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "drug_generation_final.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
